{
  "hash": "9d7021e2d01b26b98183c4ea24facfbb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Algorithmic bias\"\nformat: revealjs\n---\n\n\n\n\n\n# Garbage in, garbage out\n\n## Garbage in, garbage out\n\n-   In statistical modeling and inference we talk about \"garbage in, garbage out\" – if you don’t have good (random, representative) data, results of your analysis will not be reliable or generalizable.\n\n-   Corollary: Bias in, bias out.\n\n# Algorithmic bias and gender\n\n## Google Translate\n\n::: task\nWhat might be the reason for Google’s gendered translation?\nHow do ethics play into this situation?\n:::\n\n![](images/google-translate-gender-bias.png)\n\n::: aside\nSource: [Engadget - Google is working to remove gender bias in its translations](https://www.engadget.com/2018-12-07-google-remove-gender-bias-translations.html)\n:::\n\n## Google Translate\n\n![](images/google-translate-gender-before-after.png)\n\n::: aside\nSource: [Reducing gender bias in Google Translate](https://blog.google/products/translate/reducing-gender-bias-google-translate/)\n:::\n\n## Amazon's experimental hiring algorithm\n\n::: incremental\n-   Used AI to give job candidates scores ranging from one to five stars -- much like shoppers rate products on Amazon\n-   Amazon's system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way; it taught itself that male candidates were preferable\n:::\n\n. . .\n\n> Gender bias was not the only issue.\n> Problems with the data that underpinned the models’ judgments meant that unqualified candidates were often recommended for all manner of jobs, the people said.\n\n::: aside\nJeffrey Dastin.\n[Amazon scraps secret AI recruiting tool that showed bias against women.](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G/)\nReuters.\n10 Oct 2018.\n:::\n\n# Algorithmic bias and race\n\n## Facial recognition\n\n::::: columns\n::: {.column width=\"70%\"}\n-   Joy Buolamwini and Timnit Gebru's research identified that gender and skin-type bias in commercial artificial-intelligence systems used in facial recognition software.\n\n-   Examination of facial-analysis software showed an error rate of 0.8% for light-skinned men vs. 34.7% for dark-skinned women.\n\n-   A major U.S. technology company that designs facial recognition software claimed an accuracy rate of more than 97%, but the data set used to assess its performance was more than 77% male and more than 83% white.\n:::\n\n::: {.column width=\"30%\"}\n![](images/joy-buolamwini-mask.png)\n:::\n:::::\n\n::: aside\n[Study finds gender and skin-type bias in commercial artificial-intelligence systems](https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212)\n:::\n\n## Criminal sentencing\n\n2016 ProPublica article on algorithm used for rating a defendant's risk of future crime:\n\n![](images/machine-bias-cover.png)\n\n::: aside\nSource: Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). 23 May 2016. ProPublica.\n:::\n\n## Risk score errors\n\n::: task\nWhat is common among the defendants who were assigned a high/low risk score for reoffending?\n:::\n\n:::: columns\n::: {.column width=\"35%\"}\n![](images/machine-bias-petty-theft-1.png){fig-align=\"center\" width=\"400\" height=\"333\"} ![](images/machine-bias-petty-theft-2.png){fig-align=\"center\" width=\"400\" height=\"240\"}\n:::\n\n::: {.column width=\"35%\"}\n![](images/machine-bias-drug-posession-1.png){fig-align=\"center\" width=\"400\" height=\"333\"} ![](images/machine-bias-drug-posession-2.png){fig-align=\"center\" width=\"400\" height=\"240\"}\n:::\n::::\n\n## ProPublica analysis\n\n**Data:** Risk scores assigned to >7,000 people arrested in Broward County, FL + whether they were charged with new crimes over the following 2 years.\n\n. . .\n\n**Results:** \n\n::: incremental\n- 20% of those predicted to commit violent crimes actually did.\n- Algorithm had higher accuracy (61%) when full range of crimes taken into account (e.g. misdemeanors).\n![](images/propublica-results.png)\n- Algorithm was more likely to falsely flag black defendants as future criminals, at almost twice the rate as white defendants.\n- White defendants were mislabeled as low risk more often than black defendants.\n:::\n\n## Risk scores\n\n::: task\nHow can an algorithm that doesn't use race as input data be racist?\n:::\n\n![](images/machine-bias-risk-scores.png){fig-align=\"center\" width=\"600\"}\n\n## Predicting ethnicity {.xxsmall}\n\n[Improving Ecological Inference by Predicting Individual Ethnicity from Voter Registration Record](https://imai.fas.harvard.edu/research/race.html) (Imran and Khan, 2016)\n\n> In both political behavior research and voting rights litigation, turnout and vote choice for different racial groups are often inferred using aggregate election results and racial composition.\n> Over the past several decades, many statistical methods have been proposed to address this ecological inference problem.\n> We propose an alternative method to reduce aggregation bias by predicting individual-level ethnicity from voter registration records.\n> Building on the existing methodological literature, we use Bayes’s rule to combine the Census Bureau’s Surname List with various information from geocoded voter registration records.\n> [...]\n> We provide open-source software to implement the proposed methodology.\n> The open-source software is available for implementing the proposed methodology.\n\n## **wru** package\n\nThe said \"source software\" is the wru package: <https://github.com/kosukeimai/wru>.\n\n::: task\nDo you have any ethical concerns about installing this package?\n:::\n\n## **wru** package\n\n::: task\nWas the publication of this model ethical?\nDoes the open-source nature of the code affect your answer?\nIs it ethical to use this software?\nDoes your answer change depending on the intended use?\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wru)\npredict_race(voter.file = voters, surname.only = TRUE) |>\n  select(surname, contains(\"pred\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      surname    pred.whi    pred.bla     pred.his\n1      Khanna 0.045110474 0.003067623 0.0068522723\n2        Imai 0.052645440 0.001334812 0.0558160072\n3      Rivera 0.043285692 0.008204605 0.9136195794\n4     Fifield 0.895405704 0.001911388 0.0337464844\n5        Zhou 0.006572555 0.001298962 0.0005388581\n6    Ratkovic 0.861236727 0.008212824 0.0095395642\n7     Johnson 0.543815322 0.344128607 0.0272403940\n8       Lopez 0.038939877 0.004920643 0.9318797791\n10 Wantchekon 0.330697188 0.194700665 0.4042849478\n9       Morse 0.866360147 0.044429853 0.0246568086\n      pred.asi    pred.oth\n1  0.860411906 0.084557725\n2  0.719376581 0.170827160\n3  0.024316883 0.010573240\n4  0.011079323 0.057857101\n5  0.982365594 0.009224032\n6  0.011334635 0.109676251\n7  0.007405765 0.077409913\n8  0.012154125 0.012105576\n10 0.021379541 0.048937658\n9  0.010219712 0.054333479\n```\n\n\n:::\n:::\n\n\n\n## **wru** package\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nme <- tibble(surname = \"Çetinkaya-Rundel\")\n\npredict_race(voter.file = me, surname.only = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           surname  pred.whi pred.bla pred.his pred.asi\n1 Çetinkaya-Rundel 0.9177967        0        0        0\n    pred.oth\n1 0.08220329\n```\n\n\n:::\n:::\n\n\n\n# Further reading\n\n## Machine Bias\n\n:::: columns\n::: {.column width=\"35%\"}\n[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\\\n\nby Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner\n:::\n::: {.column width=\"35%\"}\n![](images/machine-bias-cover.png){width=\"400\"}\n:::\n::::\n\n## Ethics and Data Science\n\n:::: columns\n::: {.column width=\"35%\"}\n[Ethics and Data Science](https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7)\\\n\nby Mike Loukides, Hilary Mason, DJ Patil\\\n(Free Kindle download)\n:::\n::: {.column width=\"35%\"}\n![](images/ethics-data-science.jpg){width=\"400\"}\n:::\n::::\n\n## Weapons of Math Destruction\n\n:::: columns\n::: {.column width=\"35%\"}\n[Weapons of Math Destruction](https://www.penguin.co.uk/books/304/304513/weapons-of-math-destruction/9780141985411.html)\\\nHow Big Data Increases Inequality and Threatens Democracy\\\n\nby Cathy O'Neil\n:::\n::: {.column width=\"35%\"}\n![](images/weapons-of-math-destruction.jpg){width=\"400\"}\n:::\n::::\n\n## Algorithms of Oppression\n\n:::: columns\n::: {.column width=\"35%\"}\n[Algorithms of Oppression](https://nyupress.org/9781479837243/algorithms-of-oppression/)\\\nHow Search Engines Reinforce Racism\\\n\nby Safiya Umoja Noble\n\n<br>\n\n::: .fragment\n**More recently:** [How AI discriminates and what that means for your Google habit](https://newsroom.ucla.edu/stories/how-ai-discriminates-and-what-that-means-for-your-google-habit)\\\nA conversation with UCLA internet studies scholar Safiya Noble\\\n\nby Julia Busiek\n:::\n:::\n\n::: {.column width=\"35%\"}\n![](images/algorithms-of-oppression.jpg){width=\"400\"}\n:::\n::::\n\n## Parting thoughts\n\n-   At some point during your data science learning journey you will learn tools that can be used unethically\n\n. . .\n\n-   You might also be tempted to use your knowledge in a way that is ethically questionable either because of business goals or for the pursuit of further knowledge (or because your boss told you to do so)\n\n. . .\n\n::: task\nHow do you train yourself to make the right decisions (or reduce the likelihood of accidentally making the wrong decisions) at those points?\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}