---
title: "Algorithmic bias"
format: revealjs
---

```{r}
#| include: false
library(tidyverse)
```

# Garbage in, garbage out

## Garbage in, garbage out

-   In statistical modeling and inference we talk about "garbage in, garbage out" – if you don’t have good (random, representative) data, results of your analysis will not be reliable or generalizable.

-   Corollary: Bias in, bias out.

# Algorithmic bias and gender

## Google Translate

::: task
What might be the reason for Google’s gendered translation?
How do ethics play into this situation?
:::

![](images/google-translate-gender-bias.png)

::: aside
Source: [Engadget - Google is working to remove gender bias in its translations](https://www.engadget.com/2018-12-07-google-remove-gender-bias-translations.html)
:::

## Google Translate

![](images/google-translate-gender-before-after.png)

::: aside
Source: [Reducing gender bias in Google Translate](https://blog.google/products/translate/reducing-gender-bias-google-translate/)
:::

## Amazon's experimental hiring algorithm

::: incremental
-   Used AI to give job candidates scores ranging from one to five stars -- much like shoppers rate products on Amazon
-   Amazon's system was not rating candidates for software developer jobs and other technical posts in a gender-neutral way; it taught itself that male candidates were preferable
:::

. . .

> Gender bias was not the only issue.
> Problems with the data that underpinned the models’ judgments meant that unqualified candidates were often recommended for all manner of jobs, the people said.

::: aside
Jeffrey Dastin.
[Amazon scraps secret AI recruiting tool that showed bias against women.](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G/)
Reuters.
10 Oct 2018.
:::

# Algorithmic bias and race

## Facial recognition

::::: columns
::: {.column width="70%"}
-   Joy Buolamwini and Timnit Gebru's research identified that gender and skin-type bias in commercial artificial-intelligence systems used in facial recognition software.

-   Examination of facial-analysis software showed an error rate of 0.8% for light-skinned men vs. 34.7% for dark-skinned women.

-   A major U.S. technology company that designs facial recognition software claimed an accuracy rate of more than 97%, but the data set used to assess its performance was more than 77% male and more than 83% white.
:::

::: {.column width="30%"}
![](images/joy-buolamwini-mask.png)
:::
:::::

::: aside
[Study finds gender and skin-type bias in commercial artificial-intelligence systems](https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212)
:::

## Criminal sentencing

2016 ProPublica article on algorithm used for rating a defendant's risk of future crime:

![](images/machine-bias-cover.png)

::: aside
Source: Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner.
[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).
23 May 2016.
ProPublica.
:::

## Risk score errors

::: task
What is common among the defendants who were assigned a high/low risk score for reoffending?
:::

::::: columns
::: {.column width="35%"}
![](images/machine-bias-petty-theft-1.png){fig-align="center" width="400" height="333"} ![](images/machine-bias-petty-theft-2.png){fig-align="center" width="400" height="240"}
:::

::: {.column width="35%"}
![](images/machine-bias-drug-posession-1.png){fig-align="center" width="400" height="333"} ![](images/machine-bias-drug-posession-2.png){fig-align="center" width="400" height="240"}
:::
:::::

## ProPublica analysis

**Data:** Risk scores assigned to \>7,000 people arrested in Broward County, FL + whether they were charged with new crimes over the following 2 years.

. . .

**Results:**

::: incremental
-   20% of those predicted to commit violent crimes actually did.
-   Algorithm had higher accuracy (61%) when full range of crimes taken into account (e.g. misdemeanors). ![](images/propublica-results.png)
-   Algorithm was more likely to falsely flag black defendants as future criminals, at almost twice the rate as white defendants.
-   White defendants were mislabeled as low risk more often than black defendants.
:::

## Risk scores

::: task
How can an algorithm that doesn't use race as input data be racist?
:::

![](images/machine-bias-risk-scores.png){fig-align="center" width="600"}

## Predicting ethnicity {.xxsmall}

[Improving Ecological Inference by Predicting Individual Ethnicity from Voter Registration Record](https://imai.fas.harvard.edu/research/race.html) (Imran and Khan, 2016)

> In both political behavior research and voting rights litigation, turnout and vote choice for different racial groups are often inferred using aggregate election results and racial composition.
> Over the past several decades, many statistical methods have been proposed to address this ecological inference problem.
> We propose an alternative method to reduce aggregation bias by predicting individual-level ethnicity from voter registration records.
> Building on the existing methodological literature, we use Bayes’s rule to combine the Census Bureau’s Surname List with various information from geocoded voter registration records.
> \[...\] We provide open-source software to implement the proposed methodology.
> The open-source software is available for implementing the proposed methodology.

## **wru** package

The said "source software" is the wru package: <https://github.com/kosukeimai/wru>.

::: task
Do you have any ethical concerns about installing this package?
:::

## **wru** package

::: task
Was the publication of this model ethical?
Does the open-source nature of the code affect your answer?
Is it ethical to use this software?
Does your answer change depending on the intended use?
:::

```{r}
#| cache: true
#| message: false
library(wru)
predict_race(voter.file = voters, surname.only = TRUE) |>
  select(surname, contains("pred"))
```

# Further reading

## Machine Bias

::::: columns
::: {.column width="35%"}
[Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\

by Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner
:::

::: {.column width="35%"}
![](images/machine-bias-cover.png){width="400"}
:::
:::::

## Ethics and Data Science

::::: columns
::: {.column width="35%"}
[Ethics and Data Science](https://www.amazon.com/Ethics-Data-Science-Mike-Loukides-ebook/dp/B07GTC8ZN7)\

by Mike Loukides, Hilary Mason, DJ Patil\
(Free Kindle download)
:::

::: {.column width="35%"}
![](images/ethics-data-science.jpg){width="400"}
:::
:::::

## Weapons of Math Destruction

::::: columns
::: {.column width="35%"}
[Weapons of Math Destruction](https://www.penguin.co.uk/books/304/304513/weapons-of-math-destruction/9780141985411.html)\
How Big Data Increases Inequality and Threatens Democracy\

by Cathy O'Neil
:::

::: {.column width="35%"}
![](images/weapons-of-math-destruction.jpg){width="400"}
:::
:::::

## Algorithms of Oppression

:::::: columns
:::: {.column width="35%"}
[Algorithms of Oppression](https://nyupress.org/9781479837243/algorithms-of-oppression/)\
How Search Engines Reinforce Racism\

by Safiya Umoja Noble

<br>

::: fragment
**More recently:** [How AI discriminates and what that means for your Google habit](https://newsroom.ucla.edu/stories/how-ai-discriminates-and-what-that-means-for-your-google-habit)\
A conversation with UCLA internet studies scholar Safiya Noble\

by Julia Busiek
:::
::::

::: {.column width="35%"}
![](images/algorithms-of-oppression.jpg){width="400"}
:::
::::::

## Parting thoughts

-   At some point during your data science learning journey you will learn tools that can be used unethically

. . .

-   You might also be tempted to use your knowledge in a way that is ethically questionable either because of business goals or for the pursuit of further knowledge (or because your boss told you to do so)

. . .

::: task
How do you train yourself to make the right decisions (or reduce the likelihood of accidentally making the wrong decisions) at those points?
:::
